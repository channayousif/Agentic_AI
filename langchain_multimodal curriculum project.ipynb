{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/channayousif/Agentic_AI/blob/main/langchain_multimodal%20curriculum%20project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "812a4dbc-fe04-4b84-bdf9-390045e30806",
      "metadata": {
        "id": "812a4dbc-fe04-4b84-bdf9-390045e30806"
      },
      "source": [
        "# Multi-modal RAG with LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecXPgawqG7XH",
      "metadata": {
        "id": "ecXPgawqG7XH"
      },
      "source": [
        "## SetUp\n",
        "\n",
        "Install the dependencies you need to run the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "133b74f6",
      "metadata": {
        "id": "133b74f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d47c9c93-7339-4d22-fd76-2d83253ee2ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "libmagic-dev is already the newest version (1:5.41-3ubuntu0.1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# for linux\n",
        "!sudo apt-get install poppler-utils tesseract-ocr libmagic-dev\n",
        "\n",
        "# for mac\n",
        "# %brew install poppler tesseract libmagic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "2nDWBTrhn-_M",
      "metadata": {
        "id": "2nDWBTrhn-_M"
      },
      "outputs": [],
      "source": [
        "%pip install -Uq \"unstructured[all-docs]\" pillow lxml pillow\n",
        "%pip install -Uq chromadb tiktoken\n",
        "%pip install -Uq langchain langchain-community\n",
        "%pip install -Uq python_dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "91106e31",
      "metadata": {
        "id": "91106e31"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# keys for the services we will use\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"PINECONE_API_KEY\"] = userdata.get('PINECONE_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74b56bde-1ba0-4525-a11d-cab02c5659e4",
      "metadata": {
        "id": "74b56bde-1ba0-4525-a11d-cab02c5659e4"
      },
      "source": [
        "## Extract the data\n",
        "\n",
        "Extract the elements of the PDF that we will be able to use in the retrieval process. These elements can be: Text, Images, Tables, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e62ec070",
      "metadata": {
        "id": "e62ec070"
      },
      "source": [
        "### Partition PDF tables, text, and images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a046528-8d22-4f4e-a520-962026562939",
      "metadata": {
        "id": "0a046528-8d22-4f4e-a520-962026562939",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from unstructured.partition.pdf import partition_pdf\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "import requests\n",
        "\n",
        "output_path = \"./content/\"\n",
        "file_path = \"https://dcar.gos.pk/Sindh-Curriculum/Matrix%20for%20Curriculum%20Review%202024-25.pdf\"\n",
        "urls = [\"https://dcar.gos.pk/Sindh-Curriculum/Advanced%20Pak%20Studies%20Grade%20XI-XII%20%202021%20with%20Notified.pdf\"]\n",
        "\n",
        "# Download the PDF to a local file\n",
        "local_file_path = \"Advanced Pak Studies Grade XI-XII 2021 with Notified.pdf\"  # Choose a local file name\n",
        "response = requests.get(urls[0], stream=True)\n",
        "with open(local_file_path, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Reference: https://docs.unstructured.io/open-source/core-functionality/chunking\n",
        "chunks = partition_pdf(\n",
        "    filename=local_file_path,\n",
        "    infer_table_structure=True,            # extract tables\n",
        "    strategy=\"hi_res\",                     # mandatory to infer tables\n",
        "\n",
        "    extract_image_block_types=[\"Image\"],   # Add 'Table' to list to extract image of tables\n",
        "    image_output_dir_path=output_path,   # if None, images and tables will saved in base64\n",
        "\n",
        "    extract_image_block_to_payload=True,   # if true, will extract base64 for API usage\n",
        "\n",
        "    chunking_strategy=\"by_title\",          # or 'basic'\n",
        "    max_characters=10000,                  # defaults to 500\n",
        "    combine_text_under_n_chars=2000,       # defaults to 0\n",
        "    new_after_n_chars=6000,\n",
        "\n",
        "    # extract_images_in_pdf=True,          # deprecated\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "038f6733",
      "metadata": {
        "id": "038f6733"
      },
      "outputs": [],
      "source": [
        "# We get 2 types of elements from the partition_pdf function\n",
        "set([str(type(el)) for el in chunks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cccca0db",
      "metadata": {
        "id": "cccca0db"
      },
      "outputs": [],
      "source": [
        "# Each CompositeElement containes a bunch of related elements.\n",
        "# This makes it easy to use these elements together in a RAG pipeline.\n",
        "\n",
        "chunks[0].metadata.orig_elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8766f03",
      "metadata": {
        "id": "b8766f03"
      },
      "outputs": [],
      "source": [
        "# This is what an extracted image looks like.\n",
        "# It contains the base64 representation only because we set the param extract_image_block_to_payload=True\n",
        "\n",
        "elements = chunks[0].metadata.orig_elements\n",
        "chunk_images = [el for el in elements if 'Image' in str(type(el))]\n",
        "chunk_images[0].to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26caebda",
      "metadata": {
        "id": "26caebda"
      },
      "source": [
        "### Separate extracted elements into tables, text, and images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8326a750",
      "metadata": {
        "id": "8326a750"
      },
      "outputs": [],
      "source": [
        "# separate tables from texts\n",
        "tables = []\n",
        "texts = []\n",
        "\n",
        "for chunk in chunks:\n",
        "    if \"Table\" in str(type(chunk)):\n",
        "        tables.append(chunk)\n",
        "\n",
        "    if \"CompositeElement\" in str(type((chunk))):\n",
        "        texts.append(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df548e46",
      "metadata": {
        "id": "df548e46"
      },
      "outputs": [],
      "source": [
        "# Get the images from the CompositeElement objects\n",
        "def get_images_base64(chunks):\n",
        "    images_b64 = []\n",
        "    for chunk in chunks:\n",
        "        if \"CompositeElement\" in str(type(chunk)):\n",
        "            chunk_els = chunk.metadata.orig_elements\n",
        "            for el in chunk_els:\n",
        "                if \"Image\" in str(type(el)):\n",
        "                    images_b64.append(el.metadata.image_base64)\n",
        "    return images_b64\n",
        "\n",
        "images = get_images_base64(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9582f462",
      "metadata": {
        "id": "9582f462"
      },
      "source": [
        "#### Check what the images look like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83158c36",
      "metadata": {
        "id": "83158c36"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from IPython.display import Image, display\n",
        "\n",
        "def display_base64_image(base64_code):\n",
        "    # Decode the base64 string to binary\n",
        "    image_data = base64.b64decode(base64_code)\n",
        "    # Display the image\n",
        "    display(Image(data=image_data))\n",
        "\n",
        "display_base64_image(images[20])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aa7f52f-bf5c-4ba4-af72-b2ccba59a4cf",
      "metadata": {
        "id": "0aa7f52f-bf5c-4ba4-af72-b2ccba59a4cf"
      },
      "source": [
        "## Summarize the data\n",
        "\n",
        "Create a summary of each element extracted from the PDF. This summary will be vectorized and used in the retrieval process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b55862c",
      "metadata": {
        "id": "8b55862c"
      },
      "source": [
        "### Text and Table summaries\n",
        "\n",
        "We don't need a multimodal model to generate the summaries of the tables and the text. I will use open source models available on Groq."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b3d2bc",
      "metadata": {
        "id": "08b3d2bc"
      },
      "outputs": [],
      "source": [
        "%pip install -Uq langchain-google-genai langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "523e6ed2-2132-4748-bdb7-db765f20648d",
      "metadata": {
        "id": "523e6ed2-2132-4748-bdb7-db765f20648d"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22c22e3f-42fb-4a4a-a87a-89f10ba8ab99",
      "metadata": {
        "id": "22c22e3f-42fb-4a4a-a87a-89f10ba8ab99"
      },
      "outputs": [],
      "source": [
        "# Prompt\n",
        "prompt_text = \"\"\"\n",
        "You are an assistant tasked with summarizing tables and text.\n",
        "Give a concise summary of the table or text.\n",
        "\n",
        "Respond only with the summary, no additionnal comment.\n",
        "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
        "Just give the summary as it is.\n",
        "\n",
        "Table or text chunk: {element}\n",
        "\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
        "\n",
        "# Summary chain\n",
        "model = ChatGroq(temperature=0.5, model=\"llama-3.1-8b-instant\")\n",
        "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f176b374-aef0-48f4-a104-fb26b1dd6922",
      "metadata": {
        "id": "f176b374-aef0-48f4-a104-fb26b1dd6922"
      },
      "outputs": [],
      "source": [
        "# Summarize text\n",
        "# text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 3})\n",
        "\n",
        "import time\n",
        "\n",
        "# Summarize text with rate limiting\n",
        "text_summaries = []\n",
        "for text in texts:\n",
        "    text_summaries.append(summarize_chain.invoke(text))\n",
        "    time.sleep(5)  # Wait for 15 seconds, adjust as needed\n",
        "\n",
        "# Summarize tables with rate limiting\n",
        "tables_html = [table.metadata.text_as_html for table in tables]\n",
        "table_summaries = []\n",
        "for table_html in tables_html:\n",
        "    table_summaries.append(summarize_chain.invoke(table_html))\n",
        "    time.sleep(5)  # Wait for 15 seconds, adjust as needed\n",
        "\n",
        "\n",
        "\n",
        "# Summarize tables\n",
        "# tables_html = [table.metadata.text_as_html for table in tables]\n",
        "# table_summaries = summarize_chain.batch(tables_html, {\"max_concurrency\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d172ad2",
      "metadata": {
        "id": "1d172ad2"
      },
      "outputs": [],
      "source": [
        "text_summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1feadda-8171-4aed-9a60-320a88dc9ee1",
      "metadata": {
        "id": "b1feadda-8171-4aed-9a60-320a88dc9ee1"
      },
      "source": [
        "### Image summaries\n",
        "\n",
        "We will use gpt-4o-mini to produce the image summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c825e1",
      "metadata": {
        "id": "32c825e1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e6b1d97-4245-45ac-95ba-9bc1cfd10182",
      "metadata": {
        "id": "9e6b1d97-4245-45ac-95ba-9bc1cfd10182"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "prompt_template = \"\"\"Describe the image in detail. For context,\n",
        "                  the image is part of a research paper explaining the transformers\n",
        "                  architecture. Be specific about graphs, such as bar plots.\"\"\"\n",
        "messages = [\n",
        "    (\n",
        "        \"user\",\n",
        "        [\n",
        "            {\"type\": \"text\", \"text\": prompt_template},\n",
        "            {\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\"url\": \"data:image/jpeg;base64,{image}\"},\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "]\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "# Create the model\n",
        "generation_config = {\n",
        "  \"temperature\": .6,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 40,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "modell = ChatGoogleGenerativeAI(\n",
        "  model=\"gemini-2.0-flash\",\n",
        "  generation_config=generation_config,\n",
        ")\n",
        "\n",
        "\n",
        "chain = prompt | modell | StrOutputParser()\n",
        "\n",
        "\n",
        "# image_summaries = chain.batch(images)\n",
        "\n",
        "image_summaries = []\n",
        "for image in images:\n",
        "    image_summaries.append(chain.invoke({\"image\": image}))\n",
        "    time.sleep(7)  # Introduce a delay of 5 seconds between each request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "laF_8o1gzHT0",
      "metadata": {
        "id": "laF_8o1gzHT0"
      },
      "outputs": [],
      "source": [
        "image_summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aHNDEd_2txQI",
      "metadata": {
        "id": "aHNDEd_2txQI"
      },
      "outputs": [],
      "source": [
        "print(image_summaries[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67b030d4-2ac5-41b6-9245-fc3ba5771d87",
      "metadata": {
        "id": "67b030d4-2ac5-41b6-9245-fc3ba5771d87"
      },
      "source": [
        "## Load data and summaries to vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq langchain-pinecone\n"
      ],
      "metadata": {
        "id": "oknaSDGoTODb"
      },
      "id": "oknaSDGoTODb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bb4d2379",
      "metadata": {
        "id": "bb4d2379"
      },
      "source": [
        "### Create the vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d8d7a34-69e0-49a2-b9f7-1a4e7b26d78f",
      "metadata": {
        "id": "9d8d7a34-69e0-49a2-b9f7-1a4e7b26d78f"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.schema.document import Document\n",
        "#from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "# Pinecone vector store\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Initialize Pinecone client\n",
        "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "\n",
        "index_name = \"curriculum-project\"\n",
        "\n",
        "# Create index if it doesn't exist\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768,  # Match this to your embedding model's dimension\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n",
        "\n",
        "# Connect to the index\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Create vector store\n",
        "vectorstore = PineconeVectorStore(\n",
        "    index=index,\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "\n",
        "# The vectorstore to use to index the child chunks\n",
        "#vectorstore = Chroma(collection_name=\"multi_modal_rag\", embedding_function=embeddings)\n",
        "\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryStore()\n",
        "id_key = \"doc_id\"\n",
        "\n",
        "# The retriever (empty to start)\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    id_key=id_key,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bf26669",
      "metadata": {
        "id": "2bf26669"
      },
      "source": [
        "### Load the summaries and link the to the original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1792e683",
      "metadata": {
        "id": "1792e683"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Add texts\n",
        "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
        "summary_texts = [\n",
        "    Document(page_content=summary, metadata={id_key: doc_ids[i]}) for i, summary in enumerate(text_summaries)\n",
        "]\n",
        "retriever.vectorstore.add_documents(summary_texts)\n",
        "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
        "\n",
        "# Add tables\n",
        "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
        "summary_tables = [\n",
        "    Document(page_content=summary, metadata={id_key: table_ids[i]}) for i, summary in enumerate(table_summaries)\n",
        "]\n",
        "if summary_tables:  # Check if summary_tables is not empty\n",
        " retriever.vectorstore.add_documents(summary_tables)\n",
        " retriever.docstore.mset(list(zip(table_ids, tables)))\n",
        "\n",
        "# Add image summaries\n",
        "img_ids = [str(uuid.uuid4()) for _ in images]\n",
        "summary_img = [\n",
        "    Document(page_content=summary, metadata={id_key: img_ids[i]}) for i, summary in enumerate(image_summaries)\n",
        "]\n",
        "retriever.vectorstore.add_documents(summary_img)\n",
        "retriever.docstore.mset(list(zip(img_ids, images)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b45fb81-46b1-426e-aa2c-01aed4eac700",
      "metadata": {
        "id": "4b45fb81-46b1-426e-aa2c-01aed4eac700"
      },
      "source": [
        "### Check retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bea75fe-85af-4955-a80c-6e0b44a8e215",
      "metadata": {
        "id": "1bea75fe-85af-4955-a80c-6e0b44a8e215"
      },
      "outputs": [],
      "source": [
        "# Retrieve\n",
        "docs = retriever.invoke(\n",
        "    \"what is curriculum?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0290c78",
      "metadata": {
        "id": "a0290c78"
      },
      "outputs": [],
      "source": [
        "for doc in docs:\n",
        "    print(str(doc) + \"\\n\\n\" + \"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69060724-e390-4dda-8250-5f86025c874a",
      "metadata": {
        "id": "69060724-e390-4dda-8250-5f86025c874a"
      },
      "source": [
        "## RAG pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "771a47fa-1267-4db8-a6ae-5fde48bbc069",
      "metadata": {
        "id": "771a47fa-1267-4db8-a6ae-5fde48bbc069"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from base64 import b64decode\n",
        "\n",
        "\n",
        "def parse_docs(docs):\n",
        "    \"\"\"Split base64-encoded images and texts\"\"\"\n",
        "    b64 = []\n",
        "    text = []\n",
        "    for doc in docs:\n",
        "        try:\n",
        "            b64decode(doc)\n",
        "            b64.append(doc)\n",
        "        except Exception as e:\n",
        "            text.append(doc)\n",
        "    return {\"images\": b64, \"texts\": text}\n",
        "\n",
        "\n",
        "def build_prompt(kwargs):\n",
        "\n",
        "    docs_by_type = kwargs[\"context\"]\n",
        "    user_question = kwargs[\"question\"]\n",
        "\n",
        "    context_text = \"\"\n",
        "    if len(docs_by_type[\"texts\"]) > 0:\n",
        "        for text_element in docs_by_type[\"texts\"]:\n",
        "            context_text += text_element.text\n",
        "\n",
        "    # construct prompt with context (including images)\n",
        "    prompt_template = f\"\"\"\n",
        "    Answer the question based only on the following context, which can include text, tables, and the below image.\n",
        "    Context: {context_text}\n",
        "    Question: {user_question}\n",
        "    \"\"\"\n",
        "\n",
        "    prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
        "\n",
        "    if len(docs_by_type[\"images\"]) > 0:\n",
        "        for image in docs_by_type[\"images\"]:\n",
        "            prompt_content.append(\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            HumanMessage(content=prompt_content),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": retriever | RunnableLambda(parse_docs),\n",
        "        \"question\": RunnablePassthrough(),\n",
        "    }\n",
        "    | RunnableLambda(build_prompt)\n",
        "    | modell\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain_with_sources = {\n",
        "    \"context\": retriever | RunnableLambda(parse_docs),\n",
        "    \"question\": RunnablePassthrough(),\n",
        "} | RunnablePassthrough().assign(\n",
        "    response=(\n",
        "        RunnableLambda(build_prompt)\n",
        "        | modell\n",
        "        | StrOutputParser()\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea8414a8-65ee-4e11-8154-029b454f46af",
      "metadata": {
        "id": "ea8414a8-65ee-4e11-8154-029b454f46af"
      },
      "outputs": [],
      "source": [
        "response = chain.invoke(\n",
        "    \"What is the curriculum?\"\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4adfeba",
      "metadata": {
        "id": "e4adfeba"
      },
      "outputs": [],
      "source": [
        "response = chain_with_sources.invoke(\n",
        "    \"What is curriculum?\"\n",
        ")\n",
        "\n",
        "print(\"Response:\", response['response'])\n",
        "\n",
        "print(\"\\n\\nContext:\")\n",
        "for text in response['context']['texts']:\n",
        "    print(text.text)\n",
        "    print(\"Page number: \", text.metadata.page_number)\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "for image in response['context']['images']:\n",
        "    display_base64_image(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90cd3714",
      "metadata": {
        "id": "90cd3714"
      },
      "source": [
        "## References\n",
        "\n",
        "- [LangChain Inspiration](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb?ref=blog.langchain.dev)\n",
        "- [Multivector Storage](https://python.langchain.com/docs/how_to/multi_vector/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "74b56bde-1ba0-4525-a11d-cab02c5659e4",
        "8b55862c",
        "b1feadda-8171-4aed-9a60-320a88dc9ee1",
        "bb4d2379",
        "2bf26669",
        "4b45fb81-46b1-426e-aa2c-01aed4eac700",
        "69060724-e390-4dda-8250-5f86025c874a"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "mm-rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}